// Daniel Huang Resume Data
const danielData = {
    // Personal Information
    personal: {
        name: "Daniel Huang",
        email: "daniel.n.huang@gmail.com",
        linkedin: "linkedin.com/in/dnhuang",
        github: "github.com/dnhuang",
        title: "Software Engineer",
        location: "United States"
    },

    // ASCII Art Banner
    asciiBanner: `
 ____    _    _   _ ___ _____ _       _   _ _   _    _    _   _  ____ 
|  _ \\  / \\  | \\ | |_ _| ____| |     | | | | | | |  / \\  | \\ | |/ ___|
| | | |/ _ \\ |  \\| || ||  _| | |     | |_| | | | | / _ \\ |  \\| | |  _ 
| |_| / ___ \\| |\\  || || |___| |___  |  _  | |_| |/ ___ \\| |\\  | |_| |
|____/_/   \\_\\_| \\_|___|_____|_____| |_| |_|\\___//_/   \\_\\_| \\_|\\____|

Software Engineer | Data Science | UC Berkeley
`,

    // Welcome message
    welcomeMessage: `Welcome to Daniel's terminal resume!
Type 'help' to see available commands or 'resume' for a quick overview.
Navigate through the filesystem with 'ls', 'cd', and 'cat' commands.

guest@resume:~$ `,

    // File system structure
    filesystem: {
        "/": {
            type: "directory",
            contents: {
                "about.txt": {
                    type: "file",
                    content: `ABOUT DANIEL HUANG

Software Engineer with a strong passion in automation and all things data.
Currently modernizing healthcare data workflows at BioIntelliSense, Inc.

BACKGROUND:
‚Ä¢ UC Berkeley Data Science graduate
‚Ä¢ Experience in healthcare tech and real estate technology sectors
‚Ä¢ Driven to streamline and automate complex business processes.

CURRENT FOCUS:
‚Ä¢ Modernizing clinical data annotation workflows
‚Ä¢ Contributing to FDA clearance processes for medical devices
‚Ä¢ Developing automation tools for clinical research teams

INTERESTS:
‚Ä¢ Automation and workflow improvement
‚Ä¢ Statistical analysis and machine learning`
                },
                "contact.txt": {
                    type: "file",
                    content: `CONTACT INFORMATION

Email:     daniel.n.huang@gmail.com
LinkedIn:  linkedin.com/in/dnhuang
GitHub:    github.com/dnhuang

PROFESSIONAL PROFILES:
‚Ä¢ LinkedIn: Connect for professional networking
‚Ä¢ GitHub: Check out my code repositories and contributions
‚Ä¢ Email: Best way to reach me for opportunities

LOCATION: United States

AVAILABILITY: Open to new opportunities and interesting projects`
                },
                "skills": {
                    type: "directory",
                    contents: {
                        "programming.txt": {
                            type: "file",
                            content: `PROGRAMMING LANGUAGES & TECHNOLOGIES

LANGUAGES:
‚Ä¢ Python       - Primary language for data pipelines and automation
‚Ä¢ Java         - Enterprise applications and backend development  
‚Ä¢ SQL          - Database queries, optimization, and data analysis
‚Ä¢ C            - Systems programming and performance-critical code
‚Ä¢ Golang       - Microservices and cloud-native applications
‚Ä¢ HTML5        - Modern web markup and semantic structure
‚Ä¢ CSS          - Responsive design and user interface styling
‚Ä¢ JavaScript   - Frontend interactivity and Node.js development
‚Ä¢ XML          - Data interchange and configuration management
‚Ä¢ zsh          - Shell scripting and automation
‚Ä¢ VBA          - Office automation and macro development
‚Ä¢ Matlab       - Scientific computing and data analysis

SPECIALTIES:
‚Ä¢ ETL pipeline development and optimization
‚Ä¢ Automation script creation for non-technical users
‚Ä¢ Data visualization and dashboard creation
‚Ä¢ Performance optimization and code refactoring`
                        },
                        "databases-cloud.txt": {
                            type: "file",
                            content: `DATABASES & CLOUD TECHNOLOGIES

DATABASES:
‚Ä¢ MySQL        - Relational database design and optimization
‚Ä¢ PostgreSQL   - Advanced SQL features and performance tuning
‚Ä¢ SQLite       - Lightweight database for applications
‚Ä¢ MongoDB      - NoSQL document store for flexible schemas
‚Ä¢ Databricks   - Big data analytics and machine learning platform

CLOUD PLATFORMS:
‚Ä¢ AWS EC2      - Virtual server management and scaling
‚Ä¢ AWS S3       - Object storage and data archival
‚Ä¢ AWS Lambda   - Serverless function development

CLOUD EXPERTISE:
‚Ä¢ Infrastructure as Code (IaC) principles
‚Ä¢ Cloud-native application development
‚Ä¢ Serverless architecture design
‚Ä¢ Data lake and warehouse implementation
‚Ä¢ Cost optimization and resource management`
                        },
                        "data-science.txt": {
                            type: "file",
                            content: `DATA SCIENCE & ANALYTICS

STATISTICAL METHODS:
‚Ä¢ Hypothesis testing     - A/B testing and significance analysis
‚Ä¢ Bayesian hierarchical  - Advanced modeling for complex datasets
‚Ä¢ GLMs                   - Generalized Linear Models for prediction
‚Ä¢ Machine learning       - Supervised and unsupervised algorithms

ANALYSIS TECHNIQUES:
‚Ä¢ Multiple hypothesis testing with corrections (Bonferroni, B-H)
‚Ä¢ A/B testing framework design and implementation
‚Ä¢ Ensemble model development and validation
‚Ä¢ Time series analysis and forecasting
‚Ä¢ Correlation analysis and causality investigation

MACHINE LEARNING:
‚Ä¢ Ensemble methods (Random Forest, Gradient Boosting)
‚Ä¢ Model validation and cross-validation techniques
‚Ä¢ Feature engineering and selection
‚Ä¢ Predictive analytics for business applications
‚Ä¢ Performance metrics and model evaluation

TOOLS & FRAMEWORKS:
‚Ä¢ Scikit-learn for machine learning pipelines
‚Ä¢ Pandas for data manipulation and analysis
‚Ä¢ NumPy for numerical computing
‚Ä¢ Statistical analysis with R and Python`
                        },
                        "tools.txt": {
                            type: "file",
                            content: `DEVELOPMENT TOOLS & METHODOLOGIES

VERSION CONTROL:
‚Ä¢ Git          - Advanced branching, merging, and collaboration
‚Ä¢ GitHub       - Repository management and open source contributions
‚Ä¢ Clean version control practices and merge conflict resolution

DEVELOPMENT TOOLS:
‚Ä¢ Docker       - Containerization and deployment
‚Ä¢ Jira         - Project management and issue tracking  
‚Ä¢ Confluence   - Documentation and knowledge management
‚Ä¢ Tableau      - Business intelligence and data visualization

METHODOLOGIES:
‚Ä¢ Agile development principles and practices
‚Ä¢ Code review processes and best practices
‚Ä¢ Test-driven development (TDD)
‚Ä¢ Continuous integration and deployment (CI/CD)
‚Ä¢ Documentation-driven development

AUTOMATION & EFFICIENCY:
‚Ä¢ Custom script development for workflow automation
‚Ä¢ Macro creation for repetitive tasks
‚Ä¢ User-friendly tool development for non-technical teams
‚Ä¢ Process optimization and bottleneck identification
‚Ä¢ Performance monitoring and optimization`
                        }
                    }
                },
                "experience": {
                    type: "directory",
                    contents: {
                        "biointelligence.txt": {
                            type: "file",
                            content: `BIOINTELLIGENCE, INC.
Software Engineer | Aug 2024 - Present

COMPANY OVERVIEW:
Healthcare technology company developing FDA-cleared multi-patient 
monitoring devices and data annotation solutions.

KEY ACHIEVEMENTS:

üìä Data Annotation Modernization
‚Ä¢ Integrated Label Studio with company's AWS cloud infrastructure
‚Ä¢ Eliminated need for physical notes in clinical workflows
‚Ä¢ Significantly reduced time spent by partner physicians
‚Ä¢ Streamlined data annotation process for healthcare professionals

üè• FDA Clearance Contribution  
‚Ä¢ Refined ETL pipelines for clinical data processing
‚Ä¢ Developed data visualizations highlighting key device metrics
‚Ä¢ Contributed to FDA clearance for multi-patient use devices
‚Ä¢ Enabled new revenue growth opportunities through regulatory approval

üîß Internal Tool Development
‚Ä¢ Created portable software tools for clinical study workflows
‚Ä¢ Developed user-friendly automation scripts for research teams
‚Ä¢ Built macros to streamline repetitive clinical processes
‚Ä¢ Focused on accessibility for non-technical research staff

TECHNICAL FOCUS:
‚Ä¢ Healthcare data pipeline development
‚Ä¢ AWS cloud infrastructure integration
‚Ä¢ Clinical workflow automation
‚Ä¢ FDA-compliant data visualization
‚Ä¢ Cross-functional collaboration with medical professionals

IMPACT:
‚Ä¢ Improved efficiency of clinical data annotation workflows
‚Ä¢ Supported successful FDA device clearance process
‚Ä¢ Enhanced productivity of non-technical research teams
‚Ä¢ Contributed to company's regulatory and revenue milestones`
                        },
                        "entrust-group.txt": {
                            type: "file",
                            content: `THE ENTRUST GROUP
Real Estate Associate | Jul 2023 - Jan 2024

COMPANY OVERVIEW:
Real estate investment and transaction management company
focused on process optimization and data-driven decisions.

KEY ACHIEVEMENTS:

‚ö° ETL Pipeline Automation
‚Ä¢ Designed and implemented comprehensive ETL pipeline
‚Ä¢ Automated real estate transaction reporting processes
‚Ä¢ Reduced hundreds of manual work hours to single program run
‚Ä¢ Ensured strict adherence to business requirements and compliance

üìà Executive Reporting & Analytics
‚Ä¢ Provided KPI insights to directors and managers
‚Ä¢ Created curated visualizations for annual all-hands meeting
‚Ä¢ Developed dashboards enabling data-informed decision making
‚Ä¢ Presented complex data in accessible, actionable formats

üîÑ Administrative Automation
‚Ä¢ Wrote customized scripts for various administrative tasks
‚Ä¢ Developed programs accessible to non-technical users
‚Ä¢ Improved overall company efficiency through automation
‚Ä¢ Created user-friendly interfaces for complex processes

TECHNICAL CONTRIBUTIONS:
‚Ä¢ Real estate data pipeline architecture
‚Ä¢ Business intelligence dashboard development
‚Ä¢ Process automation for administrative workflows
‚Ä¢ Cross-departmental tool development

BUSINESS IMPACT:
‚Ä¢ Massive reduction in manual processing time
‚Ä¢ Enhanced data accuracy and consistency
‚Ä¢ Improved executive decision-making capabilities
‚Ä¢ Increased operational efficiency across departments

SKILLS DEVELOPED:
‚Ä¢ Real estate domain expertise
‚Ä¢ Business process analysis and optimization
‚Ä¢ Executive-level data presentation
‚Ä¢ User experience design for internal tools`
                        }
                    }
                },
                "projects": {
                    type: "directory",
                    contents: {
                        "fifa-predictor.txt": {
                            type: "file",
                            content: `FIFA WORLD CUP 2022 PREDICTOR
Team Leadership & Machine Learning Project

PROJECT OVERVIEW:
Led a team of seven to develop an ensemble machine learning model
for predicting FIFA World Cup 2022 winners using historical data.

TECHNOLOGIES USED:
‚Ä¢ Python for data processing and model development
‚Ä¢ Machine learning libraries (scikit-learn, pandas, numpy)
‚Ä¢ Web scraping tools for data collection
‚Ä¢ Git for version control and team collaboration

LEADERSHIP & MANAGEMENT:
üë• Team Leadership
‚Ä¢ Managed team of 7 developers and data scientists
‚Ä¢ Delegated tasks based on individual strengths and expertise
‚Ä¢ Maintained project timeline and milestone tracking
‚Ä¢ Facilitated regular team meetings and progress reviews

üîß Technical Management
‚Ä¢ Enforced clean version control practices
‚Ä¢ Minimized merge conflicts through pure function architecture
‚Ä¢ Established coding standards and review processes
‚Ä¢ Coordinated integration of different model components

DATA ENGINEERING:
üìä Data Collection & Processing
‚Ä¢ Scraped World Cup records from multiple diverse sources
‚Ä¢ Cleaned and consolidated data from various formats
‚Ä¢ Transformed raw data into machine learning-ready datasets
‚Ä¢ Handled missing data and inconsistencies across sources

MACHINE LEARNING:
ü§ñ Model Development
‚Ä¢ Trained ensemble model combining multiple algorithms
‚Ä¢ Implemented cross-validation for model reliability
‚Ä¢ Fine-tuned hyperparameters for optimal performance
‚Ä¢ Validated predictions against historical tournaments

RESULTS:
üèÜ Predictions & Validation
‚Ä¢ Predicted Argentina, Brazil, and Italy as potential winners
‚Ä¢ Argentina actually won the tournament! üá¶üá∑
‚Ä¢ Demonstrated strong model performance and team execution
‚Ä¢ Successfully delivered project on time with high accuracy

KEY LEARNINGS:
‚Ä¢ Team management in technical projects
‚Ä¢ Large-scale data integration challenges
‚Ä¢ Ensemble method implementation
‚Ä¢ Sports analytics and prediction modeling`
                        },
                        "mobility-gdp.txt": {
                            type: "file",
                            content: `MOBILITY AND US GDP RELATIONSHIP STUDY
Statistical Analysis & Bayesian Modeling Project

PROJECT OVERVIEW:
Comprehensive statistical analysis investigating the relationship
between economic indicators and population mobility patterns,
with additional research into traffic safety factors.

RESEARCH QUESTIONS:
‚Ä¢ How do GDP changes correlate with residential mobility patterns?
‚Ä¢ What is the relationship between infrastructure spending and traffic fatalities?
‚Ä¢ Can we establish statistical significance in these relationships?

STATISTICAL METHODOLOGY:
üìä Hypothesis Testing Framework
‚Ä¢ Multiple hypothesis testing with proper corrections
‚Ä¢ Naive, Bonferroni, and Benjamini-Hochberg (B-H) corrections
‚Ä¢ A/B testing methodologies for significance validation
‚Ä¢ Robust statistical inference procedures

üîç Key Findings - GDP & Mobility
‚Ä¢ Discovered significant correlation between GDP changes and residential mobility
‚Ä¢ Confirmed statistical significance through multiple testing corrections
‚Ä¢ Established robustness of findings across different analytical approaches
‚Ä¢ Demonstrated economic-demographic relationship patterns

ADVANCED MODELING:
üßÆ Bayesian Hierarchical Modeling
‚Ä¢ Constructed sophisticated Bayesian hierarchical models
‚Ä¢ Modeled complex, multi-level data structures
‚Ä¢ Incorporated prior knowledge and uncertainty quantification
‚Ä¢ Compared performance against traditional GLM approaches

‚öñÔ∏è Model Comparison & Validation
‚Ä¢ Contrasted Bayesian models with Generalized Linear Models (GLMs)
‚Ä¢ Evaluated model fit and predictive performance
‚Ä¢ Assessed assumption validity and model robustness
‚Ä¢ Interpreted results in context of domain knowledge

INFRASTRUCTURE & SAFETY ANALYSIS:
üöó Traffic Fatality Investigation
‚Ä¢ Analyzed relationship between infrastructure spending and traffic fatalities
‚Ä¢ Applied hierarchical modeling to multi-level government data
‚Ä¢ Controlled for confounding variables and regional differences
‚Ä¢ Investigated causal pathways and policy implications

TECHNICAL IMPLEMENTATION:
‚Ä¢ Python for data processing and statistical analysis
‚Ä¢ R for advanced Bayesian modeling
‚Ä¢ Statistical libraries (scipy, statsmodels, PyMC3)
‚Ä¢ Data visualization for result communication

RESEARCH IMPACT:
‚Ä¢ Demonstrated sophisticated statistical analysis capabilities
‚Ä¢ Contributed to understanding of economic-demographic relationships
‚Ä¢ Provided insights into infrastructure investment effectiveness
‚Ä¢ Showcased advanced modeling technique proficiency`
                        },
                        "gender-case-study.txt": {
                            type: "file",
                            content: `GENDER PROPER CASE USAGE ANALYSIS
A/B Testing & Data Collection Project

PROJECT INSPIRATION:
Inspired by a friendly debate among friends about writing patterns,
this project investigates gender differences in "proper case usage"
in digital communication.

RESEARCH OBJECTIVE:
Analyze whether there are statistically significant differences
in proper case usage between genders in professional contexts.

METHODOLOGY:
üî¨ A/B Testing Framework
‚Ä¢ Designed rigorous A/B testing methodology
‚Ä¢ Established control and treatment group definitions
‚Ä¢ Applied statistical significance testing
‚Ä¢ Controlled for confounding variables and biases

üìä Statistical Analysis
‚Ä¢ Implemented hypothesis testing for gender differences
‚Ä¢ Calculated effect sizes and confidence intervals
‚Ä¢ Applied multiple comparison corrections
‚Ä¢ Validated results through cross-validation techniques

DATA COLLECTION:
üï∏Ô∏è Web Scraping Operation
‚Ä¢ Scraped publicly available "bio" data from computer science course staff
‚Ä¢ Collected data across multiple semesters for temporal validation
‚Ä¢ Ensured compliance with public data access policies
‚Ä¢ Maintained data anonymization and privacy standards

üìà Dataset Characteristics
‚Ä¢ Multi-semester longitudinal data collection
‚Ä¢ Computer science academic context
‚Ä¢ Standardized bio format for consistency
‚Ä¢ Sufficient sample size for statistical power

KEY FINDINGS:
‚úÖ Primary Research Results
‚Ä¢ Established statistical significance in proper case usage differences
‚Ä¢ Demonstrated measurable gender-based communication patterns
‚Ä¢ Validated findings across multiple analytical approaches
‚Ä¢ Confirmed robustness through sensitivity analysis

üîç Peripheral Discovery
‚Ä¢ Uncovered evidence of gender hiring gap in academic settings
‚Ä¢ Identified unexpected patterns in faculty composition
‚Ä¢ Highlighted broader diversity and inclusion considerations
‚Ä¢ Demonstrated value of exploratory data analysis

TECHNICAL IMPLEMENTATION:
‚Ä¢ Python for web scraping and data processing
‚Ä¢ Statistical analysis with scipy and statsmodels
‚Ä¢ Data visualization with matplotlib and seaborn
‚Ä¢ Text processing and natural language analysis

RESEARCH SIGNIFICANCE:
‚Ä¢ Demonstrated ability to turn casual observations into rigorous research
‚Ä¢ Showcased end-to-end research project management
‚Ä¢ Applied statistical rigor to social science questions
‚Ä¢ Revealed broader implications beyond initial hypothesis

ETHICAL CONSIDERATIONS:
‚Ä¢ Used only publicly available data
‚Ä¢ Maintained participant anonymity
‚Ä¢ Followed research ethics guidelines
‚Ä¢ Considered broader implications of findings`
                        },
                        "spending-dashboard.txt": {
                            type: "file",
                            content: `PERSONAL SPENDING DASHBOARD
Data Integration & Visualization Project

PROJECT OVERVIEW:
Comprehensive personal finance analysis project involving
data consolidation from multiple financial accounts and
interactive dashboard development.

BUSINESS PROBLEM:
‚Ä¢ Lack of unified view across multiple bank accounts
‚Ä¢ Need for spending pattern identification and analysis
‚Ä¢ Desire for data-driven personal financial insights
‚Ä¢ Goal to optimize spending allocation and budgeting

DATA ENGINEERING:
üí≥ Data Consolidation
‚Ä¢ Integrated credit card transaction data from all bank accounts
‚Ä¢ Standardized data formats across different financial institutions
‚Ä¢ Cleaned and normalized transaction descriptions and categories
‚Ä¢ Handled data quality issues and duplicate transactions

üîÑ ETL Pipeline Development
‚Ä¢ Built automated data extraction processes
‚Ä¢ Implemented data transformation for analysis readiness
‚Ä¢ Created data validation and quality assurance steps
‚Ä¢ Established data refresh and update mechanisms

ANALYTICAL INSIGHTS:
üìä Spending Pattern Analysis
‚Ä¢ Categorized all transactions into meaningful spending categories
‚Ä¢ Calculated percentage allocation across different expense types
‚Ä¢ Identified seasonal and temporal spending patterns
‚Ä¢ Analyzed transaction frequency and amount distributions

üçî Key Finding - Food Expenditure Dominance
‚Ä¢ Discovered food expenditures comprised 46% of total spending
‚Ä¢ Identified this as significantly higher than expected
‚Ä¢ Revealed opportunity for budget optimization
‚Ä¢ Demonstrated value of data-driven personal insights

DASHBOARD DEVELOPMENT:
üìà Interactive Tableau Dashboard
‚Ä¢ Created comprehensive interactive visualization platform
‚Ä¢ Implemented drill-down capabilities for detailed analysis
‚Ä¢ Designed user-friendly interface for self-service analytics
‚Ä¢ Enabled dynamic filtering and time-based analysis

üéØ Dashboard Features
‚Ä¢ Monthly and yearly spending trend visualization
‚Ä¢ Category-wise spending breakdown with percentages
‚Ä¢ Transaction-level detail views and filtering
‚Ä¢ Budget vs actual spending comparison tools
‚Ä¢ Spending prediction and forecasting capabilities

TECHNICAL IMPLEMENTATION:
‚Ä¢ Data extraction from multiple banking APIs and CSV exports
‚Ä¢ Python for data processing and transformation
‚Ä¢ SQL for data storage and complex queries
‚Ä¢ Tableau for interactive dashboard development
‚Ä¢ Data validation and quality assurance processes

PERSONAL IMPACT:
‚Ä¢ Achieved clear visibility into personal spending patterns
‚Ä¢ Enabled data-driven budgeting decisions
‚Ä¢ Identified specific areas for spending optimization
‚Ä¢ Created ongoing monitoring and tracking system

PROJECT OUTCOMES:
‚úÖ Immediate Benefits
‚Ä¢ Comprehensive understanding of personal finances
‚Ä¢ Identification of largest spending category (food - 46%)
‚Ä¢ Creation of sustainable monitoring framework
‚Ä¢ Development of actionable insights for budget optimization

üöÄ Long-term Value
‚Ä¢ Ongoing dashboard for continuous financial monitoring
‚Ä¢ Framework for future financial goal tracking
‚Ä¢ Skills development in personal data analytics
‚Ä¢ Template for similar personal optimization projects`
                        }
                    }
                },
                "education": {
                    type: "directory",
                    contents: {
                        "uc-berkeley.txt": {
                            type: "file",
                            content: `UNIVERSITY OF CALIFORNIA, BERKELEY
Bachelor of Arts, Data Science | December 2022

DEGREE DETAILS:
Program: Bachelor of Arts in Data Science
Emphasis: Economics
Graduation: December 2022
Institution: University of California, Berkeley

ACADEMIC FOCUS:
üìä Data Science Core
‚Ä¢ Statistical analysis and hypothesis testing
‚Ä¢ Machine learning algorithms and applications
‚Ä¢ Data visualization and communication
‚Ä¢ Programming for data science (Python, R, SQL)

üí∞ Economics Emphasis
‚Ä¢ Microeconomic and macroeconomic theory
‚Ä¢ Econometric analysis and modeling
‚Ä¢ Economic data analysis and interpretation
‚Ä¢ Policy analysis and economic research methods

üî¨ Research & Analysis Skills
‚Ä¢ Experimental design and A/B testing
‚Ä¢ Causal inference and statistical modeling
‚Ä¢ Data collection and survey methodology
‚Ä¢ Quantitative research project management

KEY COURSEWORK:
‚Ä¢ Principles of Data Science
‚Ä¢ Statistical Methods for Data Science
‚Ä¢ Machine Learning and Statistical Learning
‚Ä¢ Economic Analysis and Policy
‚Ä¢ Econometric Analysis
‚Ä¢ Data Structures and Algorithms
‚Ä¢ Database Systems and SQL

TECHNICAL SKILLS DEVELOPED:
‚Ä¢ Python programming for data analysis
‚Ä¢ R for statistical computing
‚Ä¢ SQL for database management
‚Ä¢ Statistical software (Stata, SPSS)
‚Ä¢ Data visualization tools
‚Ä¢ Version control with Git

ACADEMIC PROJECTS:
‚Ä¢ Multi-semester data science capstone projects
‚Ä¢ Economic research with real-world datasets
‚Ä¢ Statistical analysis of policy interventions
‚Ä¢ Machine learning applications to economic problems

UC BERKELEY EXPERIENCE:
üéì Academic Excellence
‚Ä¢ Rigorous quantitative curriculum
‚Ä¢ Emphasis on practical application
‚Ä¢ Integration of theory and hands-on experience
‚Ä¢ Preparation for data-driven career paths

üåâ Campus & Community
‚Ä¢ Access to cutting-edge research facilities
‚Ä¢ Collaboration with world-class faculty
‚Ä¢ Diverse academic and professional network
‚Ä¢ Silicon Valley proximity and industry connections

PREPARATION FOR CAREER:
‚Ä¢ Strong foundation in statistical analysis
‚Ä¢ Practical experience with real-world datasets
‚Ä¢ Understanding of economic principles and applications
‚Ä¢ Technical skills in high-demand programming languages
‚Ä¢ Critical thinking and problem-solving capabilities`
                        }
                    }
                }
            }
        }
    },

    // Command help information
    commands: {
        // Standard Unix commands
        'ls': 'List directory contents',
        'cd': 'Change directory',
        'cat': 'Display file contents',
        'pwd': 'Print working directory',
        'clear': 'Clear terminal screen',
        'help': 'Display available commands',
        'history': 'Show command history',
        'tree': 'Display directory tree structure',
        
        // Custom resume commands
        'resume': 'Display complete resume overview',
        'skills': 'Show technical skills summary',
        'experience': 'List work experience',
        'projects': 'Display portfolio projects',
        'contact': 'Show contact information',
        'whoami': 'Personal introduction',
        'education': 'Show education background',
        
        // Easter egg commands
        'matrix': 'Matrix rain effect',
        'banner': 'Display ASCII art banner',
        'fortune': 'Random programming quotes',
        'cowsay': 'ASCII cow with message',
        'konami': 'Konami code easter egg'
    },

    // Fortune quotes for easter egg
    fortunes: [
        "The best way to predict the future is to implement it. - Alan Kay",
        "Any fool can write code that a computer can understand. Good programmers write code that humans can understand. - Martin Fowler",
        "First, solve the problem. Then, write the code. - John Johnson",
        "Experience is the name everyone gives to their mistakes. - Oscar Wilde",
        "In order to be irreplaceable, one must always be different. - Coco Chanel",
        "Code is like humor. When you have to explain it, it's bad. - Cory House",
        "The most important property of a program is whether it accomplishes the intention of its user. - C.A.R. Hoare",
        "Debugging is twice as hard as writing the code in the first place. - Brian Kernighan",
        "Walking on water and developing software from a specification are easy if both are frozen. - Edward V. Berard"
    ]
};

// Export for use in other modules
if (typeof module !== 'undefined' && module.exports) {
    module.exports = danielData;
}